#  Transformers

This folder contains BERT and GPT-based NLP experiments â€” including classification, generation, and transfer learning tasks. It leverages HuggingFace Transformers for quick prototyping.

---

## ðŸŽ¯ Objectives

- Explore transformer fine-tuning for NLP tasks
- Implement sentiment analysis, summarization, and text generation
- Compare BERT vs GPT architecture performance

**Note: All models are loaded from HuggingFace model hub.**

---

## ðŸ§  Notes
 - Includes pre-tokenization and batching
 - Uses pretrained BERT for classification and GPT-2 for generation
 - Evaluates F1 and BLEU scores for performance

---
