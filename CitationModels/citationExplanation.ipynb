{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73d0c01b-5828-49c2-b70a-c42926fa9089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 06:48:31.014317: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742640511.054019   23890 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742640511.073024   23890 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742640511.127873   23890 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742640511.127953   23890 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742640511.127956   23890 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742640511.127957   23890 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-22 06:48:31.144393: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1742640515.216384   23890 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built and compiled.\n",
      "\n",
      "explanationDataset loaded successfully.\n",
      "Training samples: 80000, Validation samples: 20000\n",
      "\n",
      "Tokenizers fitted.\n",
      "\n",
      "Tokenizers saved as 'encoder_tokenizer.pkl' and 'decoder_tokenizer.pkl'.\n",
      "\n",
      "Datasets created.\n",
      "\n",
      "Starting training for 10 epochs...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742640530.328889   23987 cuda_dnn.cc:529] Loaded cuDNN version 90800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 231ms/step - loss: 6.8184 - sparse_categorical_accuracy: 0.4684 - val_loss: 5.7668 - val_sparse_categorical_accuracy: 0.8049\n",
      "Epoch 2/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 230ms/step - loss: 5.6911 - sparse_categorical_accuracy: 0.8012 - val_loss: 5.3884 - val_sparse_categorical_accuracy: 0.8062\n",
      "Epoch 3/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 231ms/step - loss: 5.3060 - sparse_categorical_accuracy: 0.8008 - val_loss: 5.1592 - val_sparse_categorical_accuracy: 0.7274\n",
      "Epoch 4/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 232ms/step - loss: 5.0308 - sparse_categorical_accuracy: 0.7747 - val_loss: 4.9947 - val_sparse_categorical_accuracy: 0.7841\n",
      "Epoch 5/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 230ms/step - loss: 4.8073 - sparse_categorical_accuracy: 0.7995 - val_loss: 4.8675 - val_sparse_categorical_accuracy: 0.7859\n",
      "Epoch 6/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 229ms/step - loss: 4.6185 - sparse_categorical_accuracy: 0.7918 - val_loss: 4.7663 - val_sparse_categorical_accuracy: 0.7961\n",
      "Epoch 7/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 231ms/step - loss: 4.4528 - sparse_categorical_accuracy: 0.8009 - val_loss: 4.6819 - val_sparse_categorical_accuracy: 0.8026\n",
      "Epoch 8/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 230ms/step - loss: 4.3084 - sparse_categorical_accuracy: 0.7904 - val_loss: 4.6115 - val_sparse_categorical_accuracy: 0.7998\n",
      "Epoch 9/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 231ms/step - loss: 4.1802 - sparse_categorical_accuracy: 0.7781 - val_loss: 4.5535 - val_sparse_categorical_accuracy: 0.7951\n",
      "Epoch 10/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 229ms/step - loss: 4.0647 - sparse_categorical_accuracy: 0.7598 - val_loss: 4.5084 - val_sparse_categorical_accuracy: 0.8005\n",
      "Training complete.\n",
      "\n",
      "Model saved as 'citationExplanationModel.keras'.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# GPU Settings and Session Cleanup\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(\"Error enabling GPU memory growth:\", e)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Hyperparameters and Vocabulary Settings\n",
    "latent_dim = 256         \n",
    "embedding_dim = 128      \n",
    "max_encoder_seq_length = 500   \n",
    "max_decoder_seq_length = 100   \n",
    "num_encoder_tokens = 20000     \n",
    "num_decoder_tokens = 20000     \n",
    "\n",
    "# Build the Citation Explanation Model\n",
    "\n",
    "# Encoder: \n",
    "encoder_inputs = Input(shape=(None,), name=\"encoder_inputs\")\n",
    "encoder_embedding = Embedding(num_encoder_tokens, embedding_dim, mask_zero=True, name=\"encoder_embedding\")(encoder_inputs)\n",
    "encoder_lstm, state_h, state_c = LSTM(latent_dim, return_state=True, name=\"encoder_lstm\")(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder: \n",
    "decoder_inputs = Input(shape=(None,), name=\"decoder_inputs\")\n",
    "decoder_embedding = Embedding(num_decoder_tokens, embedding_dim, mask_zero=True, name=\"decoder_embedding\")(decoder_inputs)\n",
    "decoder_lstm, _, _ = LSTM(latent_dim, return_sequences=True, return_state=True, name=\"decoder_lstm\")(\n",
    "    decoder_embedding, initial_state=encoder_states)\n",
    "decoder_outputs = Dense(num_decoder_tokens, activation='softmax', name=\"decoder_dense\")(decoder_lstm)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "print(\"Model built and compiled.\\n\")\n",
    "\n",
    "# Data Generator for Large Datasets\n",
    "def data_generator(encoder_texts, decoder_texts, batch_size, encoder_tokenizer, decoder_tokenizer):\n",
    "    num_samples = len(encoder_texts)\n",
    "    while True:\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            enc_batch = encoder_texts[offset:offset+batch_size]\n",
    "            dec_batch = decoder_texts[offset:offset+batch_size]\n",
    "            \n",
    "            # Tokenize texts to sequences\n",
    "            enc_sequences = encoder_tokenizer.texts_to_sequences(enc_batch)\n",
    "            dec_sequences = decoder_tokenizer.texts_to_sequences(dec_batch)\n",
    "            \n",
    "            # Pad sequences to fixed lengths\n",
    "            enc_sequences = pad_sequences(enc_sequences, maxlen=max_encoder_seq_length, padding='post')\n",
    "            dec_sequences = pad_sequences(dec_sequences, maxlen=max_decoder_seq_length, padding='post')\n",
    "            \n",
    "            # Prepare decoder target data (shifted by one timestep)\n",
    "            dec_target = np.zeros_like(dec_sequences)\n",
    "            dec_target[:, :-1] = dec_sequences[:, 1:]\n",
    "            \n",
    "            # Yield as a tuple of tuples to match output_signature\n",
    "            yield ((enc_sequences, dec_sequences), dec_target[..., np.newaxis])\n",
    "\n",
    "# Load Dataset, Prepare Data, and Train Model\n",
    "if __name__ == \"__main__\":\n",
    "    data = pd.read_csv(\"explanationDataset.csv\")\n",
    "    print(\"explanationDataset loaded successfully.\")\n",
    "    \n",
    "    all_encoder_texts = []\n",
    "    all_decoder_texts = []\n",
    "    for idx, row in data.iterrows():\n",
    "        encoder_text = f\"{row['context_before']} {row['context_after']} {row['cited_paper_title']} {row['cited_paper_abstract']}\"\n",
    "        all_encoder_texts.append(encoder_text)\n",
    "        all_decoder_texts.append(row['citation_sentence'])\n",
    "    \n",
    "    train_enc, val_enc, train_dec, val_dec = train_test_split(\n",
    "        all_encoder_texts, all_decoder_texts, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"Training samples: {len(train_enc)}, Validation samples: {len(val_enc)}\\n\")\n",
    "    \n",
    "    # Fit tokenizers on the training set\n",
    "    encoder_tokenizer = Tokenizer(num_words=num_encoder_tokens, oov_token=\"<OOV>\")\n",
    "    decoder_tokenizer = Tokenizer(num_words=num_decoder_tokens, oov_token=\"<OOV>\")\n",
    "    encoder_tokenizer.fit_on_texts(train_enc)\n",
    "    decoder_tokenizer.fit_on_texts(train_dec)\n",
    "    print(\"Tokenizers fitted.\\n\")\n",
    "    \n",
    "    # Save tokenizers for later inference demo\n",
    "    with open(\"encoder_tokenizer.pkl\", \"wb\") as f:\n",
    "        pickle.dump(encoder_tokenizer, f)\n",
    "    with open(\"decoder_tokenizer.pkl\", \"wb\") as f:\n",
    "        pickle.dump(decoder_tokenizer, f)\n",
    "    print(\"Tokenizers saved as 'encoder_tokenizer.pkl' and 'decoder_tokenizer.pkl'.\\n\")\n",
    "    \n",
    "    batch_size = 64  \n",
    "    steps_per_epoch = len(train_enc) // batch_size\n",
    "    validation_steps = len(val_enc) // batch_size\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(train_enc, train_dec, batch_size, encoder_tokenizer, decoder_tokenizer),\n",
    "        output_signature=(\n",
    "            (\n",
    "                tf.TensorSpec(shape=(None, max_encoder_seq_length), dtype=tf.int32),\n",
    "                tf.TensorSpec(shape=(None, max_decoder_seq_length), dtype=tf.int32)\n",
    "            ),\n",
    "            tf.TensorSpec(shape=(None, max_decoder_seq_length, 1), dtype=tf.int32)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(val_enc, val_dec, batch_size, encoder_tokenizer, decoder_tokenizer),\n",
    "        output_signature=(\n",
    "            (\n",
    "                tf.TensorSpec(shape=(None, max_encoder_seq_length), dtype=tf.int32),\n",
    "                tf.TensorSpec(shape=(None, max_decoder_seq_length), dtype=tf.int32)\n",
    "            ),\n",
    "            tf.TensorSpec(shape=(None, max_decoder_seq_length, 1), dtype=tf.int32)\n",
    "        )\n",
    "    )\n",
    "    print(\"Datasets created.\\n\")\n",
    "    \n",
    "    epochs = 10  \n",
    "    print(f\"Starting training for {epochs} epochs...\")\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_dataset,\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    print(\"Training complete.\\n\")\n",
    "    \n",
    "    # Save the model in the required .keras format\n",
    "    model.save(\"citationExplanationModel.keras\")\n",
    "    print(\"Model saved as 'citationExplanationModel.keras'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
