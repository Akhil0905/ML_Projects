{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc4cd771-583a-48c2-aacb-6205532f9eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 06:10:09.690469: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742638209.704511   21405 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742638209.710002   21405 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742638209.725883   21405 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742638209.725933   21405 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742638209.725936   21405 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742638209.725938   21405 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-22 06:10:09.730946: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All warnings and unnecessary logs suppressed successfully!\n",
      "Loaded cleaned dataset with 26000 samples.\n",
      "Encoding texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742638258.712748   21405 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded input shape: (26000, 128), Attention mask shape: (26000, 128)\n",
      "Training the model...\n",
      "Epoch 1/5\n",
      "1300/1300 [==============================] - 356s 262ms/step - loss: 0.6958 - accuracy: 0.4982 - val_loss: 0.6934 - val_accuracy: 0.4975\n",
      "Epoch 2/5\n",
      "1300/1300 [==============================] - 340s 261ms/step - loss: 0.6942 - accuracy: 0.5004 - val_loss: 0.6934 - val_accuracy: 0.4975\n",
      "Epoch 3/5\n",
      "1300/1300 [==============================] - 339s 261ms/step - loss: 0.6937 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4973\n",
      "Epoch 4/5\n",
      "1300/1300 [==============================] - 339s 261ms/step - loss: 0.6918 - accuracy: 0.5156 - val_loss: 0.6953 - val_accuracy: 0.5010\n",
      "Epoch 5/5\n",
      "1300/1300 [==============================] - 337s 259ms/step - loss: 0.6746 - accuracy: 0.5713 - val_loss: 0.7126 - val_accuracy: 0.4981\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, create_optimizer, logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tf_keras.callbacks import EarlyStopping\n",
    "\n",
    "# Suppress warnings and logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\", message=\".*overflowing tokens are not returned.*\")\n",
    "print(\"All warnings and unnecessary logs suppressed successfully!\")\n",
    "\n",
    "# Load the cleaned dataset\n",
    "data = pd.read_csv(\"recommendationDataset.csv\")\n",
    "print(f\"Loaded cleaned dataset with {len(data)} samples.\")\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Encode the inputs in batches to ensure consistent length\n",
    "print(\"Encoding texts...\")\n",
    "encoded_inputs = tokenizer(\n",
    "    data['citing_sentence'].tolist(),\n",
    "    data['cited_paper_abstract'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=128,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "# Convert the encoded inputs to NumPy arrays\n",
    "input_ids = np.array(encoded_inputs['input_ids'])\n",
    "attention_masks = np.array(encoded_inputs['attention_mask'])\n",
    "\n",
    "print(f\"Encoded input shape: {input_ids.shape}, Attention mask shape: {attention_masks.shape}\")\n",
    "\n",
    "# Labels (convert to numpy array)\n",
    "labels = np.array(data['label'].astype(int).tolist())\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
    "    input_ids, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_masks, val_masks = train_test_split(\n",
    "    attention_masks, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create TensorFlow dataset objects\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "    \"input_ids\": train_inputs,\n",
    "    \"attention_mask\": train_masks\n",
    "}, train_labels)).batch(16)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "    \"input_ids\": val_inputs,\n",
    "    \"attention_mask\": val_masks\n",
    "}, val_labels)).batch(16)\n",
    "\n",
    "# Load the pre-trained BERT model for binary classification\n",
    "model = TFBertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=2\n",
    ")\n",
    "\n",
    "# Create optimizer from Hugging Face\n",
    "num_train_steps = len(train_dataset) * 5  # 5 epochs\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    ")\n",
    "\n",
    "# Compile the model using the created optimizer\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=3, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=5,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"citationRecommendationModel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
